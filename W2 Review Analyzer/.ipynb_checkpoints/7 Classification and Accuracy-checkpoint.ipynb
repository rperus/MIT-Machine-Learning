{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b28b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation, digits\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef245c",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb1155",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    A classification function that uses theta and theta_0 to classify a set of\n",
    "    data points.\n",
    "\n",
    "    Args:\n",
    "        feature_matrix - A numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "                theta - A numpy array describing the linear classifier.\n",
    "        theta - A numpy array describing the linear classifier.\n",
    "        theta_0 - A real valued number representing the offset parameter.\n",
    "\n",
    "    Returns: A numpy array of 1s and -1s where the kth element of the array is\n",
    "    the predicted classification of the kth row of the feature matrix using the\n",
    "    given theta and theta_0. If a prediction is GREATER THAN zero, it should\n",
    "    be considered a positive classification.\n",
    "    \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c781d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(feature_matrix, theta, theta_0):\n",
    "    \n",
    "    X=feature_matrix\n",
    "    w=theta\n",
    "    w0=theta_0\n",
    "    pred = []\n",
    "        \n",
    "    for i in range(len(feature_matrix)):\n",
    "        \n",
    "        if (np.dot(X[i], w)+w0)>0:\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(-1)       \n",
    "    return np.array(pred)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af560264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(feature_matrix, theta, theta_0):\n",
    "    (nsamples, nfeatures) = feature_matrix.shape\n",
    "    predictions = np.zeros(nsamples)\n",
    "    for i in range(nsamples):\n",
    "        feature_vector = feature_matrix[i]\n",
    "        prediction = np.dot(theta, feature_vector) + theta_0\n",
    "        if (prediction > 0):\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = -1\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc8417da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(feature_matrix, theta, theta_0):\n",
    "    \n",
    "    predictions=[]\n",
    "    for i in range(len(feature_matrix)):\n",
    "        \n",
    "       if(np.dot(theta,feature_matrix[i].transpose())+theta_0)>0:\n",
    "           \n",
    "           predictions.append(1)\n",
    "       else:\n",
    "           predictions.append(-1)\n",
    "           \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43caba1",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "866f7aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGiven length-N vectors containing predicted and target labels,\\nreturns the percentage and number of correct predictions.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given length-N vectors containing predicted and target labels,\n",
    "returns the percentage and number of correct predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e492db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, targets):\n",
    "    return (preds == targets).mean()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Trains a linear classifier and computes accuracy.\n",
    "    The classifier is trained on the train data. The classifier's\n",
    "    accuracy on the train and validation data is then returned.\n",
    "\n",
    "    Args:\n",
    "        classifier - A classifier function that takes arguments\n",
    "            (feature matrix, labels, **kwargs) and returns (theta, theta_0)\n",
    "        train_feature_matrix - A numpy matrix describing the training\n",
    "            data. Each row represents a single data point.\n",
    "        val_feature_matrix - A numpy matrix describing the validation\n",
    "            data. Each row represents a single data point.\n",
    "        train_labels - A numpy array where the kth element of the array\n",
    "            is the correct classification of the kth row of the training\n",
    "            feature matrix.\n",
    "        val_labels - A numpy array where the kth element of the array\n",
    "            is the correct classification of the kth row of the validation\n",
    "            feature matrix.\n",
    "        **kwargs - Additional named arguments to pass to the classifier\n",
    "            (e.g. T or L)\n",
    "\n",
    "    Returns: A tuple in which the first element is the (scalar) accuracy of the\n",
    "    trained classifier on the training data and the second element is the\n",
    "    accuracy of the trained classifier on the validation data.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7171679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_single_step_update(\n",
    "        feature_vector,\n",
    "        label,\n",
    "        current_theta,\n",
    "        current_theta_0):\n",
    "    X=feature_vector\n",
    "    Y=label\n",
    "    w=current_theta\n",
    "    w0=current_theta_0\n",
    "\n",
    "\n",
    "    if (np.dot(X, w)+w0)*Y <= 0:\n",
    "                w = w + X*Y\n",
    "                w0 = w0 + Y\n",
    "              \n",
    "\n",
    "    return (w,w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66d0ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(feature_matrix, labels, **kwargs):\n",
    "\n",
    "    \n",
    "    # Your code here\n",
    "   \n",
    "    X=feature_matrix\n",
    "    Y=labels\n",
    "    w = np.zeros(len(X[0]))\n",
    "    \n",
    "    #w = np.transpose([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    w0 = 0\n",
    "    for t in range(**kwargs):\n",
    "        for i in get_order(feature_matrix.shape[0]):\n",
    "             w,w0=perceptron_single_step_update(X[i,:], Y[i], w, w0)\n",
    "    return (w,w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b772b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_accuracy(\n",
    "        classifier,\n",
    "        train_feature_matrix,\n",
    "        val_feature_matrix,\n",
    "        train_labels,\n",
    "        val_labels,\n",
    "        **kwargs):\n",
    "    \n",
    "    X_train=train_feature_matrix\n",
    "    X_test=val_feature_matrix\n",
    "    y_train=train_labels\n",
    "    y_test=val_labels\n",
    "    \n",
    "    w, w0 = classifier(X_train, y_train, **kwargs)\n",
    "    pred_train=classify(X_train,w,w0)\n",
    "    pred_val = classify(X_test, w, w0)\n",
    "    \n",
    "    acc_train=accuracy(pred_train,y_train)\n",
    "    acc_test=accuracy(pred_val,y_test)\n",
    "    \n",
    "    return (acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d3bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e078dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b04e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0525de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ej1\n",
    "#https://www.kaggle.com/code/jvdahemad/building-linear-classifier-models-from-scratch/notebook\n",
    "def classifier_accuracy(classifier,train_feature_matrix,val_feature_matrix,train_labels,val_labels,**kwargs):\n",
    "\n",
    "    thetas=classifier(train_feature_matrix,train_labels,**kwargs)\n",
    "        \n",
    "    predictions_train=classify(train_feature_matrix,thetas[0],thetas[1])\n",
    "    predictions_val=classify(val_feature_matrix,thetas[0],thetas[1])\n",
    "    \n",
    "    train_acc=accuracy(predictions_train,train_labels)\n",
    "    val_acc=accuracy(predictions_val,val_labels)\n",
    "    \n",
    "    return (train_acc,val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc3b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ej2\n",
    "#https://github.com/mmbajo/Machine-Learning-Perceptrons/blob/master/project1.py\n",
    "def classifier_accuracy(classifier,train_feature_matrix,val_feature_matrix,train_labels,val_labels,**kwargs):\n",
    "\n",
    "    theta, theta_0 = classifier(train_feature_matrix, train_labels, **kwargs)\n",
    "    train_pred = classify(train_feature_matrix, theta, theta_0)\n",
    "    train_accuracy = accuracy(train_pred, train_labels)\n",
    "    val_pred = classify(val_feature_matrix, theta, theta_0)\n",
    "    val_accuracy = accuracy(val_pred, val_labels)\n",
    "    return (train_accuracy, val_accuracy)\n",
    "    #raise NotImplementedError\n",
    "#pragma: coderesponse end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a84057",
   "metadata": {},
   "source": [
    "# Baseline Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "807ccb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation, digits\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def get_order(n_samples):\n",
    "    try:\n",
    "        with open(str(n_samples) + '.txt') as fp:\n",
    "            line = fp.readline()\n",
    "            return list(map(int, line.split(',')))\n",
    "    except FileNotFoundError:\n",
    "        random.seed(1)\n",
    "        indices = list(range(n_samples))\n",
    "        random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "\n",
    "# def hinge_loss_single(feature_vector, label, theta, theta_0):\n",
    "\n",
    "#     z = label*(np.sum(np.multiply(feature_vector,theta))+theta_0)\n",
    "#     ret = max(0,1-z) \n",
    "#     return ret\n",
    "#     raise NotImplementedError\n",
    "\n",
    "\n",
    "# def hinge_loss_full(feature_matrix, labels, theta, theta_0):\n",
    "#     return np.maximum(0, 1 - labels*(np.sum(feature_matrix * theta, axis = 1) + theta_0)).mean()\n",
    "\n",
    "\n",
    "def perceptron_single_step_update(\n",
    "        feature_vector,\n",
    "        label,\n",
    "        current_theta,\n",
    "        current_theta_0):\n",
    "    X=feature_vector\n",
    "    Y=label\n",
    "    w=current_theta\n",
    "    w0=current_theta_0\n",
    "\n",
    "    if (np.dot(X, w)+w0)*Y <= 0:\n",
    "        w = w + X*Y\n",
    "        w0 = w0 + Y              \n",
    "    return (w,w0)\n",
    "\n",
    "\n",
    "\n",
    "def perceptron(feature_matrix, labels, T):\n",
    "\n",
    "    \n",
    "    # Your code here\n",
    "    X=feature_matrix\n",
    "    Y=labels\n",
    "    w = np.zeros(len(X[0]))\n",
    "    w0 = 0\n",
    "    for t in range(T):\n",
    "        for i in get_order(feature_matrix.shape[0]):\n",
    "             w,w0=perceptron_single_step_update(X[i,:], Y[i], w, w0)\n",
    "    return (w,w0)\n",
    "\n",
    "#             pass\n",
    "#     raise NotImplementedError\n",
    "\n",
    "\n",
    "def average_perceptron(feature_matrix, labels, T):\n",
    "\n",
    "    X=feature_matrix\n",
    "    Y=labels\n",
    "    w = np.zeros(len(X[0]))\n",
    "    w0 = 0\n",
    "    wnew= np.zeros(len(X[0]))\n",
    "    w0new=0\n",
    "    counter=0\n",
    "    for t in range(T):\n",
    "        for i in get_order(feature_matrix.shape[0]):\n",
    "            w,w0=perceptron_single_step_update(X[i,:], Y[i], w, w0)\n",
    "            wnew= wnew + w\n",
    "            w0new= w0new + w0\n",
    "            counter= counter + 1           \n",
    "    return (wnew/counter,w0new/counter)\n",
    "\n",
    "\n",
    "def pegasos_single_step_update(\n",
    "        feature_vector,\n",
    "        label,\n",
    "        L,\n",
    "        eta,\n",
    "        current_theta,\n",
    "        current_theta_0):\n",
    "\n",
    "    X=feature_vector\n",
    "    Y=label\n",
    "    w=current_theta\n",
    "    w0=current_theta_0\n",
    "\n",
    "\n",
    "    if (np.dot(X, w)+w0)*Y <= 1:\n",
    "        w = w*(1-L*eta) + X*Y*eta\n",
    "        w0 = w0 + Y*eta\n",
    "    else:\n",
    "        w = w*(1-L*eta)\n",
    "        w0 = w0\n",
    "    return (w,w0)\n",
    "\n",
    "\n",
    "def pegasos(feature_matrix, labels, T, L):\n",
    "    X=feature_matrix\n",
    "    Y=labels\n",
    "    w = np.zeros(len(X[0]))\n",
    "    w0 = 0\n",
    "    counter= 0\n",
    "    (nvectors, dimensions) = feature_matrix.shape\n",
    "    for t in range(T):\n",
    "\n",
    "        for i in get_order(nvectors):        \n",
    "            counter=counter+1\n",
    "            eta = 1/np.sqrt(counter)\n",
    "            w,w0=pegasos_single_step_update(X[i,:], Y[i],L, eta, w, w0)\n",
    "\n",
    "    return (w,w0)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f66c0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bag_of_words(texts):\n",
    "    \"\"\"\n",
    "    Inputs a list of string reviews\n",
    "    Returns a dictionary of unique unigrams occurring over the input\n",
    "\n",
    "    Feel free to change this code as guided by Problem 9\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    dictionary = {} # maps word to unique index\n",
    "    for text in texts:\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            if word not in dictionary:\n",
    "                dictionary[word] = len(dictionary)\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def extract_bow_feature_vectors(reviews, dictionary):\n",
    "    \"\"\"\n",
    "    Inputs a list of string reviews\n",
    "    Inputs the dictionary of words as given by bag_of_words\n",
    "    Returns the bag-of-words feature matrix representation of the data.\n",
    "    The returned matrix is of shape (n, m), where n is the number of reviews\n",
    "    and m the total number of entries in the dictionary.\n",
    "\n",
    "    Feel free to change this code as guided by Problem 9\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "\n",
    "    num_reviews = len(reviews)\n",
    "    feature_matrix = np.zeros([num_reviews, len(dictionary)])\n",
    "\n",
    "    for i, text in enumerate(reviews):\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            if word in dictionary:\n",
    "                feature_matrix[i, dictionary[word]] = 1\n",
    "    return feature_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5ddde89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_data, extras=False):\n",
    "    \"\"\"\n",
    "    Returns a list of dict with keys:\n",
    "    * sentiment: +1 or -1 if the review was positive or negative, respectively\n",
    "    * text: the text of the review\n",
    "\n",
    "    Additionally, if the `extras` argument is True, each dict will also include the\n",
    "    following information:\n",
    "    * productId: a string that uniquely identifies each product\n",
    "    * userId: a string that uniquely identifies each user\n",
    "    * summary: the title of the review\n",
    "    * helpfulY: the number of users who thought this review was helpful\n",
    "    * helpfulN: the number of users who thought this review was NOT helpful\n",
    "    \"\"\"\n",
    "\n",
    "    global PYTHON3\n",
    "\n",
    "    basic_fields = {'sentiment', 'text'}\n",
    "    numeric_fields = {'sentiment', 'helpfulY', 'helpfulN'}\n",
    "\n",
    "    data = []\n",
    "    if PYTHON3:\n",
    "        f_data = open(path_data, encoding=\"latin1\")\n",
    "    else:\n",
    "        f_data = open(path_data)\n",
    "\n",
    "    for datum in csv.DictReader(f_data, delimiter='\\t'):\n",
    "        for field in list(datum.keys()):\n",
    "            if not extras and field not in basic_fields:\n",
    "                del datum[field]\n",
    "            elif field in numeric_fields and datum[field]:\n",
    "                datum[field] = int(datum[field])\n",
    "\n",
    "        data.append(datum)\n",
    "\n",
    "    f_data.close()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68277d21",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PYTHON3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35568/1560616800.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# test_data = utils.load_data('reviews_test.tsv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reviews_train.tsv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mval_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reviews_val.tsv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reviews_test.tsv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35568/3668649272.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(path_data, extras)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mPYTHON3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mf_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"latin1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PYTHON3' is not defined"
     ]
    }
   ],
   "source": [
    "# import utils\n",
    "# import numpy as np\n",
    "# import project1 as p1\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Data loading. There is no need to edit code in this section.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# train_data = utils.load_data('reviews_train.tsv')\n",
    "# val_data = utils.load_data('reviews_val.tsv')\n",
    "# test_data = utils.load_data('reviews_test.tsv')\n",
    "\n",
    "train_data = load_data('reviews_train.tsv')\n",
    "val_data = load_data('reviews_val.tsv')\n",
    "test_data = load_data('reviews_test.tsv')\n",
    "\n",
    "train_texts, train_labels = zip(*((sample['text'], sample['sentiment']) for sample in train_data))\n",
    "val_texts, val_labels = zip(*((sample['text'], sample['sentiment']) for sample in val_data))\n",
    "test_texts, test_labels = zip(*((sample['text'], sample['sentiment']) for sample in test_data))\n",
    "\n",
    "dictionary = bag_of_words(train_texts)\n",
    "\n",
    "train_bow_features = extract_bow_feature_vectors(train_texts, dictionary)\n",
    "val_bow_features = extract_bow_feature_vectors(val_texts, dictionary)\n",
    "test_bow_features = extract_bow_feature_vectors(test_texts, dictionary)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Problem 5\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# toy_features, toy_labels = toy_data = utils.load_toy_data('toy_data.tsv')\n",
    "\n",
    "# T = 10\n",
    "# L = 0.2\n",
    "\n",
    "# thetas_perceptron = p1.perceptron(toy_features, toy_labels, T)\n",
    "# thetas_avg_perceptron = p1.average_perceptron(toy_features, toy_labels, T)\n",
    "# thetas_pegasos = p1.pegasos(toy_features, toy_labels, T, L)\n",
    "\n",
    "# def plot_toy_results(algo_name, thetas):\n",
    "#     print('theta for', algo_name, 'is', ', '.join(map(str,list(thetas[0]))))\n",
    "#     print('theta_0 for', algo_name, 'is', str(thetas[1]))\n",
    "#     utils.plot_toy_data(algo_name, toy_features, toy_labels, thetas)\n",
    "\n",
    "# plot_toy_results('Perceptron', thetas_perceptron)\n",
    "# plot_toy_results('Average Perceptron', thetas_avg_perceptron)\n",
    "# plot_toy_results('Pegasos', thetas_pegasos)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Problem 7\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "T = 10\n",
    "L = 0.01\n",
    "\n",
    "pct_train_accuracy, pct_val_accuracy = \\\n",
    "   classifier_accuracy(perceptron, train_bow_features,val_bow_features,train_labels,val_labels,T=T)\n",
    "print(\"{:35} {:.4f}\".format(\"Training accuracy for perceptron:\", pct_train_accuracy))\n",
    "print(\"{:35} {:.4f}\".format(\"Validation accuracy for perceptron:\", pct_val_accuracy))\n",
    "\n",
    "# avg_pct_train_accuracy, avg_pct_val_accuracy = \\\n",
    "#    classifier_accuracy(average_perceptron, train_bow_features,val_bow_features,train_labels,val_labels,T=T)\n",
    "# print(\"{:43} {:.4f}\".format(\"Training accuracy for average perceptron:\", avg_pct_train_accuracy))\n",
    "# print(\"{:43} {:.4f}\".format(\"Validation accuracy for average perceptron:\", avg_pct_val_accuracy))\n",
    "\n",
    "# avg_peg_train_accuracy, avg_peg_val_accuracy = \\\n",
    "#    classifier_accuracy(pegasos, train_bow_features,val_bow_features,train_labels,val_labels,T=T,L=L)\n",
    "# print(\"{:50} {:.4f}\".format(\"Training accuracy for Pegasos:\", avg_peg_train_accuracy))\n",
    "# print(\"{:50} {:.4f}\".format(\"Validation accuracy for Pegasos:\", avg_peg_val_accuracy))\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Problem 8\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# data = (train_bow_features, train_labels, val_bow_features, val_labels)\n",
    "#\n",
    "# # values of T and lambda to try\n",
    "# Ts = [1, 5, 10, 15, 25, 50]\n",
    "# Ls = [0.001, 0.01, 0.1, 1, 10]\n",
    "#\n",
    "# pct_tune_results = utils.tune_perceptron(Ts, *data)\n",
    "# print('perceptron valid:', list(zip(Ts, pct_tune_results[1])))\n",
    "# print('best = {:.4f}, T={:.4f}'.format(np.max(pct_tune_results[1]), Ts[np.argmax(pct_tune_results[1])]))\n",
    "#\n",
    "# avg_pct_tune_results = utils.tune_avg_perceptron(Ts, *data)\n",
    "# print('avg perceptron valid:', list(zip(Ts, avg_pct_tune_results[1])))\n",
    "# print('best = {:.4f}, T={:.4f}'.format(np.max(avg_pct_tune_results[1]), Ts[np.argmax(avg_pct_tune_results[1])]))\n",
    "#\n",
    "# # fix values for L and T while tuning Pegasos T and L, respective\n",
    "# fix_L = 0.01\n",
    "# peg_tune_results_T = utils.tune_pegasos_T(fix_L, Ts, *data)\n",
    "# print('Pegasos valid: tune T', list(zip(Ts, peg_tune_results_T[1])))\n",
    "# print('best = {:.4f}, T={:.4f}'.format(np.max(peg_tune_results_T[1]), Ts[np.argmax(peg_tune_results_T[1])]))\n",
    "#\n",
    "# fix_T = Ts[np.argmax(peg_tune_results_T[1])]\n",
    "# peg_tune_results_L = utils.tune_pegasos_L(fix_T, Ls, *data)\n",
    "# print('Pegasos valid: tune L', list(zip(Ls, peg_tune_results_L[1])))\n",
    "# print('best = {:.4f}, L={:.4f}'.format(np.max(peg_tune_results_L[1]), Ls[np.argmax(peg_tune_results_L[1])]))\n",
    "#\n",
    "# utils.plot_tune_results('Perceptron', 'T', Ts, *pct_tune_results)\n",
    "# utils.plot_tune_results('Avg Perceptron', 'T', Ts, *avg_pct_tune_results)\n",
    "# utils.plot_tune_results('Pegasos', 'T', Ts, *peg_tune_results_T)\n",
    "# utils.plot_tune_results('Pegasos', 'L', Ls, *peg_tune_results_L)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Use the best method (perceptron, average perceptron or Pegasos) along with\n",
    "# the optimal hyperparameters according to validation accuracies to test\n",
    "# against the test dataset. The test data has been provided as\n",
    "# test_bow_features and test_labels.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Your code here\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Assign to best_theta, the weights (and not the bias!) learned by your most\n",
    "# accurate algorithm with the optimal choice of hyperparameters.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# best_theta = None # Your code here\n",
    "# wordlist   = [word for (idx, word) in sorted(zip(dictionary.values(), dictionary.keys()))]\n",
    "# sorted_word_features = utils.most_explanatory_word(best_theta, wordlist)\n",
    "# print(\"Most Explanatory Word Features\")\n",
    "# print(sorted_word_features[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5bfa94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9268dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
